{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "704c5dd8-cff3-4c82-833e-b8466f83466a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text in lowercase with removed punctuation is\n",
      " technology has transformed the way we live work and communicate \n",
      "from smartphones to artificial intelligence innovation continues to shape our future \n",
      "the internet connects people across the globe in an instant \n",
      "automation and machine learning are changing industries and creating new opportunities \n",
      "while technology can sometimes feel overwhelming it offers incredible tools for solving realworld problems\n",
      "\n",
      "Word tokenization\n",
      " ['technology', 'has', 'transformed', 'the', 'way', 'we', 'live', 'work', 'and', 'communicate', 'from', 'smartphones', 'to', 'artificial', 'intelligence', 'innovation', 'continues', 'to', 'shape', 'our', 'future', 'the', 'internet', 'connects', 'people', 'across', 'the', 'globe', 'in', 'an', 'instant', 'automation', 'and', 'machine', 'learning', 'are', 'changing', 'industries', 'and', 'creating', 'new', 'opportunities', 'while', 'technology', 'can', 'sometimes', 'feel', 'overwhelming', 'it', 'offers', 'incredible', 'tools', 'for', 'solving', 'realworld', 'problems']\n",
      "\n",
      "Sentence tokenization\n",
      " ['Technology has transformed the way we live, work, and communicate.', 'From smartphones to artificial intelligence, innovation continues to shape our future.', 'The internet connects people across the globe in an instant.', 'Automation and machine learning are changing industries and creating new opportunities.', 'While technology can sometimes feel overwhelming, it offers incredible tools for solving real-world problems.']\n",
      "\n",
      "Word tokenization\n",
      " ['technology', 'has', 'transformed', 'the', 'way', 'we', 'live', ',', 'work', ',', 'and', 'communicate', '.', 'from', 'smartphones', 'to', 'artificial', 'intelligence', ',', 'innovation', 'continues', 'to', 'shape', 'our', 'future', '.', 'the', 'internet', 'connects', 'people', 'across', 'the', 'globe', 'in', 'an', 'instant', '.', 'automation', 'and', 'machine', 'learning', 'are', 'changing', 'industries', 'and', 'creating', 'new', 'opportunities', '.', 'while', 'technology', 'can', 'sometimes', 'feel', 'overwhelming', ',', 'it', 'offers', 'incredible', 'tools', 'for', 'solving', 'real-world', 'problems', '.']\n",
      "\n",
      " Split using python function\n",
      " ['technology', 'has', 'transformed', 'the', 'way', 'we', 'live,', 'work,', 'and', 'communicate.', 'from', 'smartphones', 'to', 'artificial', 'intelligence,', 'innovation', 'continues', 'to', 'shape', 'our', 'future.', 'the', 'internet', 'connects', 'people', 'across', 'the', 'globe', 'in', 'an', 'instant.', 'automation', 'and', 'machine', 'learning', 'are', 'changing', 'industries', 'and', 'creating', 'new', 'opportunities.', 'while', 'technology', 'can', 'sometimes', 'feel', 'overwhelming,', 'it', 'offers', 'incredible', 'tools', 'for', 'solving', 'real-world', 'problems.']\n",
      "\n",
      "Text after removing stop words is\n",
      " ['technology', 'transformed', 'way', 'live', 'work', 'communicate', 'smartphones', 'artificial', 'intelligence', 'innovation', 'continues', 'shape', 'future', 'internet', 'connects', 'people', 'across', 'globe', 'instant', 'automation', 'machine', 'learning', 'changing', 'industries', 'creating', 'new', 'opportunities', 'technology', 'sometimes', 'feel', 'overwhelming', 'offers', 'incredible', 'tools', 'solving', 'realworld', 'problems']\n",
      "\n",
      "Word Frequency Counts\n",
      "\n",
      "technology: 2\n",
      "transformed: 1\n",
      "way: 1\n",
      "live: 1\n",
      "work: 1\n",
      "communicate: 1\n",
      "smartphones: 1\n",
      "artificial: 1\n",
      "intelligence: 1\n",
      "innovation: 1\n",
      "continues: 1\n",
      "shape: 1\n",
      "future: 1\n",
      "internet: 1\n",
      "connects: 1\n",
      "people: 1\n",
      "across: 1\n",
      "globe: 1\n",
      "instant: 1\n",
      "automation: 1\n",
      "machine: 1\n",
      "learning: 1\n",
      "changing: 1\n",
      "industries: 1\n",
      "creating: 1\n",
      "new: 1\n",
      "opportunities: 1\n",
      "sometimes: 1\n",
      "feel: 1\n",
      "overwhelming: 1\n",
      "offers: 1\n",
      "incredible: 1\n",
      "tools: 1\n",
      "solving: 1\n",
      "realworld: 1\n",
      "problems: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "import string;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize;\n",
    "from nltk.probability import FreqDist;\n",
    "nltk.download('punkt');\n",
    "nltk.download('stopwords');\n",
    "\n",
    "paragraph = \"\"\"Technology has transformed the way we live, work, and communicate. \n",
    "From smartphones to artificial intelligence, innovation continues to shape our future. \n",
    "The internet connects people across the globe in an instant. \n",
    "Automation and machine learning are changing industries and creating new opportunities. \n",
    "While technology can sometimes feel overwhelming, it offers incredible tools for solving real-world problems.\"\"\";\n",
    "\n",
    "text_lower = paragraph.lower();\n",
    "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation));\n",
    "print(\"Text in lowercase with removed punctuation is\\n\",text_clean);\n",
    "\n",
    "words = word_tokenize(text_clean);\n",
    "print(\"\\nWord tokenization\\n\",words);\n",
    "sentences = sent_tokenize(paragraph);\n",
    "print(\"\\nSentence tokenization\\n\",sentences);\n",
    "\n",
    "word = word_tokenize(text_lower);\n",
    "print(\"\\nWord tokenization\\n\",word);\n",
    "split = text_lower.split();\n",
    "print(\"\\n Split using python function\\n\",split);\n",
    "\n",
    "stop_words = set(stopwords.words('english'));\n",
    "filtered_words = [word for word in words if word not in stop_words];\n",
    "print(\"\\nText after removing stop words is\\n\",filtered_words);\n",
    "\n",
    "freq_dist = FreqDist(filtered_words)\n",
    "print(\"\\nWord Frequency Counts\\n\");\n",
    "for word, freq in freq_dist.items():\n",
    "    print(f\"{word}: {freq}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe46998-8363-48da-ae06-6d9a426a5697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words with only alphabets (lowercase):\n",
      " ['technology', 'is', 'evolving', 'faster', 'than', 'ever', 'before', 'smart', 'devices', 'are', 'becoming', 'essential', 'parts', 'of', 'our', 'daily', 'lives', 'artificial', 'intelligence', 'helps', 'businesses', 'automate', 'tasks', 'and', 'make', 'smarter', 'decisions', 'virtual', 'reality', 'is', 'changing', 'the', 'way', 'we', 'experience', 'games', 'movies', 'and', 'education', 'even', 'healthcare', 'is', 'being', 'transformed', 'through', 'wearable', 'tech', 'and', 'data', 'driven', 'diagnostics']\n",
      "\n",
      "Filtered Words (after removing stopwords):\n",
      " ['technology', 'evolving', 'faster', 'ever', 'smart', 'devices', 'becoming', 'essential', 'parts', 'daily', 'lives', 'artificial', 'intelligence', 'helps', 'businesses', 'automate', 'tasks', 'make', 'smarter', 'decisions', 'virtual', 'reality', 'changing', 'way', 'experience', 'games', 'movies', 'education', 'even', 'healthcare', 'transformed', 'wearable', 'tech', 'data', 'driven', 'diagnostics']\n",
      "\n",
      "Stemmed Words (using PorterStemmer):\n",
      " ['technolog', 'evolv', 'faster', 'ever', 'smart', 'devic', 'becom', 'essenti', 'part', 'daili', 'live', 'artifici', 'intellig', 'help', 'busi', 'autom', 'task', 'make', 'smarter', 'decis', 'virtual', 'realiti', 'chang', 'way', 'experi', 'game', 'movi', 'educ', 'even', 'healthcar', 'transform', 'wearabl', 'tech', 'data', 'driven', 'diagnost']\n",
      "\n",
      "Lemmatized Words (using WordNetLemmatizer):\n",
      " ['technology', 'evolving', 'faster', 'ever', 'smart', 'device', 'becoming', 'essential', 'part', 'daily', 'life', 'artificial', 'intelligence', 'help', 'business', 'automate', 'task', 'make', 'smarter', 'decision', 'virtual', 'reality', 'changing', 'way', 'experience', 'game', 'movie', 'education', 'even', 'healthcare', 'transformed', 'wearable', 'tech', 'data', 'driven', 'diagnostics']\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "import re;\n",
    "from nltk.corpus import stopwords;\n",
    "from nltk.tokenize import word_tokenize;\n",
    "from nltk.stem import PorterStemmer;\n",
    "from nltk.stem import WordNetLemmatizer;\n",
    "nltk.download('punkt');\n",
    "nltk.download('stopwords');\n",
    "nltk.download('wordnet');\n",
    "\n",
    "paragraph = \"\"\"Technology is evolving faster than ever before. \n",
    "Smart devices are becoming essential parts of our daily lives. \n",
    "Artificial Intelligence helps businesses automate tasks and make smarter decisions. \n",
    "Virtual Reality is changing the way we experience games, movies, and education. \n",
    "Even healthcare is being transformed through wearable tech and data-driven diagnostics.\"\"\";\n",
    "\n",
    "words_alpha = re.findall(r'\\b[a-zA-Z]+\\b', paragraph.lower());\n",
    "print(\"Words with only alphabets (lowercase):\\n\", words_alpha)\n",
    "\n",
    "stop_words = set(stopwords.words('english'));\n",
    "filtered_words = [word for word in words_alpha if word not in stop_words];\n",
    "print(\"\\nFiltered Words (after removing stopwords):\\n\", filtered_words);\n",
    "\n",
    "porter_stemmer = PorterStemmer();\n",
    "stemmed_words = [porter_stemmer.stem(word) for word in filtered_words];\n",
    "print(\"\\nStemmed Words (using PorterStemmer):\\n\", stemmed_words);\n",
    "\n",
    "lemmatizer = WordNetLemmatizer();\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words];\n",
    "print(\"\\nLemmatized Words (using WordNetLemmatizer):\\n\", lemmatized_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac07fb7c-ac00-468e-81d4-87fcf7fafd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Representation:\n",
      "[[0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 1]\n",
      " [1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0]\n",
      " [0 1 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0]]\n",
      "Feature Names (Words in BoW): ['2025' 'across' 'amazing' 'best' 'breaking' 'causes' 'change' 'damage'\n",
      " 'day' 'disaster' 'every' 'features' 'here' 'innovations' 'is' 'natural'\n",
      " 'new' 'news' 'of' 'offering' 'region' 'smartphone' 'tech' 'the'\n",
      " 'widespread' 'world']\n",
      "TF-IDF Scores: \n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.36888498 0.         0.36888498 0.         0.36888498 0.\n",
      "  0.         0.36888498 0.         0.         0.36888498 0.\n",
      "  0.         0.         0.         0.         0.36888498 0.21786941\n",
      "  0.         0.36888498]\n",
      " [0.32705548 0.         0.32705548 0.32705548 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.32705548\n",
      "  0.32705548 0.         0.32705548 0.         0.         0.\n",
      "  0.32705548 0.32705548 0.         0.32705548 0.         0.19316423\n",
      "  0.         0.        ]\n",
      " [0.         0.32705548 0.         0.         0.32705548 0.32705548\n",
      "  0.         0.32705548 0.         0.32705548 0.         0.\n",
      "  0.         0.         0.         0.32705548 0.         0.32705548\n",
      "  0.         0.         0.32705548 0.         0.         0.19316423\n",
      "  0.32705548 0.        ]]\n",
      "Feature Names (Words in TF-IDF):  ['2025' 'across' 'amazing' 'best' 'breaking' 'causes' 'change' 'damage'\n",
      " 'day' 'disaster' 'every' 'features' 'here' 'innovations' 'is' 'natural'\n",
      " 'new' 'news' 'of' 'offering' 'region' 'smartphone' 'tech' 'the'\n",
      " 'widespread' 'world']\n",
      "\n",
      "Top 3 Keywords (TF-IDF Scores) for each Text:\n",
      "\n",
      "Text 1: New tech innovations change the world every day.\n",
      "Top 3 Keywords (Word, TF-IDF Score):\n",
      "'world': 0.3689\n",
      "'every': 0.3689\n",
      "'tech': 0.3689\n",
      "\n",
      "Text 2: The best smartphone of 2025 is here, offering amazing features.\n",
      "Top 3 Keywords (Word, TF-IDF Score):\n",
      "'here': 0.3271\n",
      "'of': 0.3271\n",
      "'amazing': 0.3271\n",
      "\n",
      "Text 3: Breaking news: Natural disaster causes widespread damage across the region.\n",
      "Top 3 Keywords (Word, TF-IDF Score):\n",
      "'disaster': 0.3271\n",
      "'across': 0.3271\n",
      "'region': 0.3271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer;\n",
    "import numpy as np;\n",
    "\n",
    "texts = [\n",
    "    \"New tech innovations change the world every day.\",\n",
    "    \"The best smartphone of 2025 is here, offering amazing features.\",\n",
    "    \"Breaking news: Natural disaster causes widespread damage across the region.\"\n",
    "];\n",
    "\n",
    "vectorizer = CountVectorizer();\n",
    "X_count = vectorizer.fit_transform(texts);\n",
    "count_feature_names = vectorizer.get_feature_names_out();\n",
    "print(\"Bag of Words Representation:\");\n",
    "print(X_count.toarray());\n",
    "print(\"Feature Names (Words in BoW):\", count_feature_names);\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(texts);\n",
    "print(\"TF-IDF Scores: \");\n",
    "print(X_tfidf.toarray());\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out();\n",
    "print(\"Feature Names (Words in TF-IDF): \",tfidf_feature_names);\n",
    "\n",
    "print(\"\\nTop 3 Keywords (TF-IDF Scores) for each Text:\");\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"\\nText {i + 1}: {text}\");\n",
    "    tfidf_scores = X_tfidf[i].toarray().flatten();\n",
    "    top_indices = tfidf_scores.argsort()[-3:][::-1];\n",
    "    top_keywords = [(tfidf_feature_names[idx], tfidf_scores[idx]) for idx in top_indices];\n",
    "    print(\"Top 3 Keywords (Word, TF-IDF Score):\");\n",
    "    for keyword, score in top_keywords:\n",
    "        print(f\"'{keyword}': {score:.4f}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c883f57d-7496-44a9-aa36-ab8afe69b00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for AI: ['artificial', 'intelligence', 'ai', 'is', 'revolutionizing', 'industries', 'by', 'automating', 'tasks', 'and', 'improving', 'decisionmaking', 'it', 'uses', 'machine', 'learning', 'algorithms', 'to', 'process', 'large', 'datasets', 'and', 'make', 'predictions', 'ai', 'applications', 'range', 'from', 'selfdriving', 'cars', 'to', 'voice', 'assistants', 'and', 'it', 'continues', 'to', 'evolve', 'rapidly'] \n",
      "\n",
      "Tokens for Blockchain: ['blockchain', 'is', 'a', 'decentralized', 'ledger', 'technology', 'that', 'ensures', 'secure', 'transactions', 'without', 'intermediaries', 'it', 'is', 'most', 'commonly', 'known', 'for', 'being', 'the', 'backbone', 'of', 'cryptocurrencies', 'like', 'bitcoin', 'blockchains', 'transparency', 'and', 'security', 'make', 'it', 'a', 'powerful', 'tool', 'for', 'various', 'industries', 'from', 'finance', 'to', 'supply', 'chains']\n",
      "\n",
      "Jaccard Similarity: 0.109375\n",
      "\n",
      "Cosine Similarity: 0.1772053000124084\n",
      "\n",
      "Cosine Similarity gives better insights in this case.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "import re;\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer;\n",
    "from sklearn.metrics.pairwise import cosine_similarity;\n",
    "nltk.download('punkt');\n",
    "\n",
    "text_ai = \"\"\"Artificial Intelligence (AI) is revolutionizing industries by automating tasks and improving decision-making. \n",
    "It uses machine learning algorithms to process large datasets and make predictions. AI applications range from self-driving cars to voice assistants, \n",
    "and it continues to evolve rapidly.\"\"\";\n",
    "text_blockchain = \"\"\"Blockchain is a decentralized ledger technology that ensures secure transactions without intermediaries. \n",
    "It is most commonly known for being the backbone of cryptocurrencies like Bitcoin. Blockchain’s transparency and security make it a powerful tool for various industries, \n",
    "from finance to supply chains.\"\"\";\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower(); \n",
    "    text = re.sub(r'[^\\w\\s]', '', text);\n",
    "    tokens = nltk.word_tokenize(text);\n",
    "    return tokens;\n",
    "tokens_ai = preprocess_text(text_ai);\n",
    "print(\"Tokens for AI:\", tokens_ai, \"\\n\");\n",
    "tokens_blockchain = preprocess_text(text_blockchain);\n",
    "print(\"Tokens for Blockchain:\", tokens_blockchain);\n",
    "\n",
    "def jaccard_similarity(tokens1, tokens2):\n",
    "    set1, set2 = set(tokens1), set(tokens2);\n",
    "    intersection = set1.intersection(set2);\n",
    "    union = set1.union(set2);\n",
    "    return len(intersection) / len(union);\n",
    "\n",
    "jaccard_sim = jaccard_similarity(tokens_ai, tokens_blockchain)\n",
    "print(\"\\nJaccard Similarity:\", jaccard_sim);\n",
    "\n",
    "vectorizer = TfidfVectorizer();\n",
    "texts = [text_ai, text_blockchain];\n",
    "tfidf_matrix = vectorizer.fit_transform(texts);\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1]);\n",
    "print(\"\\nCosine Similarity:\", cosine_sim[0][0]);\n",
    "\n",
    "if jaccard_sim > cosine_sim:\n",
    "    print(\"\\nJaccard Similarity gives better insights in this case.\");\n",
    "else:\n",
    "    print(\"\\nCosine Similarity gives better insights in this case.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b324de1a-7399-4660-940e-c50949b5d439",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob;\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvaderSentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvaderSentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer;\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud;\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob;\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer;\n",
    "from wordcloud import WordCloud;\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "review = \"I recently bought the XYZ smartphone, and I am really impressed with its performance! The battery lasts all day, and the camera quality is outstanding. The display is vibrant and smooth, making it perfect for gaming and media consumption. Although the price is a bit high, I think it's totally worth it for all the features it offers. Highly recommend it!\";\n",
    "\n",
    "blob = TextBlob(review);\n",
    "textblob_polarity = blob.sentiment.polarity;\n",
    "print(\"TextBlob Polarity:\", textblob_polarity);\n",
    "textblob_subjectivity = blob.sentiment.subjectivity;\n",
    "print(\"TextBlob Subjectivity:\", textblob_subjectivity);\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer();\n",
    "vader_sentiment = analyzer.polarity_scores(review);\n",
    "print(\"\\nVADER Sentiment Scores:\", vader_sentiment);\n",
    "def classify_review(polarity_score):\n",
    "    if polarity_score > 0.1:\n",
    "        return \"Positive\";\n",
    "    elif polarity_score < -0.1:\n",
    "        return \"Negative\";\n",
    "    else:\n",
    "        return \"Neutral\";\n",
    "classification = classify_review(textblob_polarity);\n",
    "print(\"\\nReview Classification (TextBlob):\", classification);\n",
    "\n",
    "positive_reviews = [review];\n",
    "positive_reviews_text = \" \".join(positive_reviews);\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews_text);\n",
    "plt.figure(figsize=(10, 5));\n",
    "plt.imshow(wordcloud, interpolation='bilinear');\n",
    "plt.axis('off');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c129b2-9e96-4bbc-ac5e-3422d07f06ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
